from pathlib import Path
from typing import Generator

import torch
from matchms import Spectrum
from torch.nn import Transformer

from .model.generative import CSTseq, tkn
from .utils.spectra import tensorize_peaks


class Predictor:
    def __init__(self, ckpt: str | Path = Path(__file__).parent / "ckpt" / "seq.ckpt", device: str = "cpu"):
        """
        Initialize the Predictor
        :param ckpt: Path to the pt file
        :param device: Device to run the model on
        """
        ckpt = Path(ckpt)
        self.seq = CSTseq.load_from_checkpoint(ckpt).eval().to(device)
        self.device = device

    @staticmethod
    def _make_peaks(spec: Spectrum):
        return tensorize_peaks(
            spec.peak_comments,
            spec.peaks.intensities,
            spec.get("formula"),
            spec.get("adduct")
        ).float().unsqueeze(1)

    def _generate_lookahead_mask(self, size: int) -> torch.Tensor:
        return Transformer.generate_square_subsequent_mask(size, device=torch.device(self.device))

    @torch.no_grad()
    def _beam_search_decode(self, embedding, hint, max_len, beam_width, alpha):
        ys = hint
        eos_id = tkn.token_to_id("</s>")

        # Initialize beam with the start token
        beam = [(ys, 0)]

        for _ in range(max_len):
            candidates = []
            for ys, score in beam:
                # Check if the last token in the sequence is the end token
                if ys[-1].item() == eos_id:
                    candidates.append((ys, score))
                else:
                    tgt_mask = self._generate_lookahead_mask(ys.shape[0])
                    out = self.seq.decode(ys, embedding, tgt_mask)[-1]
                    probs, indices = torch.topk(
                        torch.softmax(out, dim=1), beam_width)
                    for i in range(beam_width):
                        next_token = torch.ones(1, 1).fill_(
                            indices[0, i]).type(torch.long)
                        new_ys = torch.cat([ys, next_token], dim=0)
                        new_score = (score * ((ys.shape[0] - 1) ** alpha) + torch.log(probs[0, i])) / (
                                ys.shape[0] ** alpha)
                        candidates.append((new_ys, new_score))

                # Sort the candidates by their scores and keep only the top beam_width sequences
            candidates.sort(key=lambda x: x[1], reverse=True)
            beam = candidates[:beam_width]

            # Check if all candidate sequences end with the end token
            all_eos = all(c[0][-1].item() == eos_id for c in beam)
            if all_eos:
                break
        return beam

    @torch.no_grad()
    def beam_predict(self, spec: Spectrum, n_beam=5, alpha=0.75) -> list[tuple[str, float]]:
        """
        Predict the SMILES of the given spectrum using beam search algorithm
        :param spec: Spectrum to be annotated
        :param n_beam: Number of beams to be used
        :param alpha: Length normalization factor
        :return: List of predicted SMILES and their scores
        """
        hint = torch.tensor(tkn.encode(
            spec.get("standard_formula")).ids, dtype=torch.long).unsqueeze(1)
        peaks = self._make_peaks(spec)
        tgt_tokens = self._beam_search_decode(
            self.seq.encode(peaks), hint, max_len=200, beam_width=n_beam, alpha=alpha)
        results = [("".join((tkn.decode(indices.squeeze(1).numpy())).split(
            " ")), score.item()) for indices, score in tgt_tokens]
        return [(smiles[len(spec.get("standard_formula")):], score) for smiles, score in results]

    @torch.no_grad()
    def sample_prob(self, spec: Spectrum, smiles: str) -> float:
        """
        Calculate the probability of the given SMILES string given the spectrum
        :param spec: Spectrum to be annotated
        :param smiles: candidate SMILES string
        :return: Probability of the SMILES string to be generated by the model
        """
        tgt_tokens = tkn.encode(spec.get("standard_formula"), smiles)
        tgt_in = torch.tensor(
            tgt_tokens.ids, dtype=torch.long).unsqueeze(1)[:-1]
        tgt_mask = self._generate_lookahead_mask(tgt_in.shape[0])
        peaks = self._make_peaks(spec)
        embedding = self.seq.encode(peaks)
        logits = self.seq.decode(tgt_in, embedding, tgt_mask)
        segment = torch.tensor(tgt_tokens.type_ids, dtype=bool)
        probs = torch.softmax(logits, dim=-1)[segment[1:]].squeeze(1)
        smiles_id = torch.tensor(tgt_tokens.ids, dtype=torch.long)[segment]
        # probs shape: (seq_len, vocab_size)
        # smiles_id shape: (seq_len)
        smiles_prob = probs[torch.arange(probs.shape[0]), smiles_id]
        return smiles_prob.mean().item()

    @torch.no_grad()
    def _greedy_decode(self, embedding, hint, max_len):
        """
        Generator version of the greedy decode algorithm.
        Yields the token sequences as they are generated.

        :param embedding: The encoded peaks from the model.
        :param hint: The starting tokens (e.g., formula tokens).
        :param max_len: The maximum number of tokens to generate.
        """
        ys = hint
        for _ in range(max_len):
            tgt_mask = self._generate_lookahead_mask(ys.shape[0])
            out = self.seq.decode(ys, embedding, tgt_mask)[-1]
            _, idx = torch.max(out, dim=1)
            ys = torch.cat([ys, idx.unsqueeze(1)], dim=0)

            # Yield the current sequence after adding the new token
            yield idx.unsqueeze(1)

            # Stop decoding if the end token is generated
            if idx.item() == tkn.token_to_id("</s>"):
                break

    @torch.no_grad()
    def greedy_stream(self, spec: Spectrum) -> Generator[str, None, None]:
        """
        Streamed version of greedy prediction that yields intermediate SMILES strings.
        :param spec: Spectrum to be annotated.
        :yield: Partially predicted SMILES string at each step.
        """
        hint = torch.tensor(tkn.encode(spec.get("standard_formula")).ids, dtype=torch.long).unsqueeze(1)
        peaks = self._make_peaks(spec)

        for step in self._greedy_decode(self.seq.encode(peaks), hint, max_len=200):
            # Decode the intermediate token sequence into SMILES
            result = tkn.decode(step.squeeze(1).numpy())
            # Yield the intermediate SMILES string (removing spaces)
            yield "".join(result.split(" "))

    def __call__(self, spec: Spectrum) -> Generator[str, None, None]:
        """
        Predict the SMILES of the given spectrum using greedy search algorithm
        :param spec: Spectrum to be annotated
        :yield: Predicted SMILES string
        """
        return self.greedy_stream(spec)
