"""
TIFF collection to OME-ZARR
===========================

This script converts a collection of tiff files generated by the LSM
pipeline into a OME-ZARR or NIFTI-ZARR pyramid.

Example input files can be found at
https://lincbrain.org/dandiset/000004/0.240319.1924/files?location=derivatives%2F

dependencies:
    numpy
    tifffile
    zarr
    nibabel
    cyclopts
"""
import cyclopts
import zarr
import os
import re
import ast
import numpy as np
import nibabel as nib
from tifffile import TiffFile
from glob import glob
from typing import Optional, List
# local modules
from utils import \
    make_compressor, ceildiv, orientation_to_affine, center_affine


app = cyclopts.App(help_format="markdown")


@app.default
def convert(
    inp: str,
    out: str = None,
    *,
    chunk: int = 128,
    compressor: str = 'blosc',
    compressor_opt: str = "{}",
    max_load: int = 512,
    nii: bool = False,
    orientation: str = 'coronal',
    center: bool = True,
    thickness: Optional[float] = None,
    voxel_size: List[float] = (1, 1, 1),
):
    """
    This script converts a collection of tiff files generated by the LSM
    pipeline into a OME-ZARR or NIFTI-ZARR pyramid.

    ## Orientation
    The anatomical orientation of the slice is given in terms of RAS axes.
    It is a combination of two letters from the set {L, R, A, P, I, S}, where

    - the first letter corresponds to the horizontal dimension and
        indicates the anatomical meaning of the _right_ of the jp2 image,
    - the second letter corresponds to the vertical dimension and
        indicates the anatomical meaning of the _bottom_ of the jp2 image.

    We also provide the aliases

    - coronal == LI
    - axial == LP
    - sagittal == PI

    The orientation flag is only useful when converting to nifti-zarr.

    Parameters
    ----------
    inp
        Path to the root directory, which contains a collection of
        subfolders named `*_z{:02d}_y{:02d}*`, each containing a
        collection of files named `*_plane{:03d}_c{:d}.tiff`.
    out
        Path to the output Zarr directory [<INP>.ome.zarr]
    chunk
        Output chunk size
    compressor : {blosc, zlib, raw}
        Compression method
    compressor_opt
        Compression options
    max_load
        Maximum input chunk size when building pyramid
    nii
        Convert to nifti-zarr. True if path ends in ".nii.zarr".
    orientation
        Orientation of the slice
    center
        Set RAS[0, 0, 0] at FOV center
    voxel_size
        Voxel size along the X, Y and Z dimension, in micron.
    """
    if isinstance(compressor_opt, str):
        compressor_opt = ast.literal_eval(compressor_opt)

    if max_load % 2:
        max_load += 1

    CHUNK_PATTERN = re.compile(
        r'^(?P<prefix>\w*)'
        r'_z(?P<z>[0-9]+)'
        r'_y(?P<y>[0-9]+)'
        r'(?P<suffix>\w*)$'
    )

    all_chunks_dirnames = list(sorted(glob(os.path.join(inp, '*_z*_y*'))))
    all_chunks_info = dict(
        dirname=[],
        prefix=[],
        suffix=[],
        z=[],
        y=[],
        planes=[
            dict(
                fname=[],
                z=[],
                c=[],
                yx_shape=[],
            )
            for _ in range(len(all_chunks_dirnames))
        ],
    )

    # parse all directory names
    for dirname in all_chunks_dirnames:
        parsed = CHUNK_PATTERN.fullmatch(os.path.basename(dirname))
        all_chunks_info['dirname'].append(dirname)
        all_chunks_info['prefix'].append(parsed.group('prefix'))
        all_chunks_info['suffix'].append(parsed.group('suffix'))
        all_chunks_info['z'].append(int(parsed.group('z')))
        all_chunks_info['y'].append(int(parsed.group('y')))

    # default output name
    if not out:
        out = all_chunks_info['prefix'][0] + all_chunks_info['suffix'][0]
        out += '.nii.zarr' if nii else '.ome.zarr'
    nii = nii or out.endswith('.nii.zarr')

    # parse all individual file names
    nchunkz = max(all_chunks_info['z'])
    nchunky = max(all_chunks_info['y'])
    allshapes = [[(0, 0, 0) for _ in range(nchunky)] for _ in range(nchunkz)]
    nchannels = 0
    dtype = None
    for zchunk in range(nchunkz):
        for ychunk in range(nchunky):
            for i in range(len(all_chunks_info['dirname'])):
                if all_chunks_info['z'][i] == zchunk + 1 \
                        and all_chunks_info['y'][i] == ychunk + 1:
                    break
            dirname = all_chunks_info['dirname'][i]
            planes_filenames \
                = list(sorted(glob(os.path.join(dirname, '*.tiff'))))

            PLANE_PATTERN = re.compile(
                os.path.basename(dirname) +
                r'_plane(?P<z>[0-9]+)'
                r'_c(?P<c>[0-9]+)'
                r'.tiff$'
            )

            for fname in planes_filenames:
                parsed = PLANE_PATTERN.fullmatch(os.path.basename(fname))
                all_chunks_info['planes'][i]['fname'].append(fname)
                all_chunks_info['planes'][i]['z'].append(int(parsed.group('z')))
                all_chunks_info['planes'][i]['c'].append(int(parsed.group('c')))

                f = TiffFile(fname)
                dtype = f.pages[0].dtype
                yx_shape = f.pages[0].shape
                all_chunks_info['planes'][i]['yx_shape'].append(yx_shape)

            nplanes = max(all_chunks_info['planes'][i]['z'])
            nchannels = max(nchannels, max(all_chunks_info['planes'][i]['c']))

            yx_shape = set(all_chunks_info['planes'][i]['yx_shape'])
            if not len(yx_shape) == 1:
                raise ValueError('Incompatible chunk shapes')
            yx_shape = list(yx_shape)[0]
            allshapes[zchunk][ychunk] = (nplanes, *yx_shape)

    # check that all chink shapes are compatible
    for zchunk in range(nchunkz):
        if len(set(shape[1] for shape in allshapes[zchunk])) != 1:
            raise ValueError('Incompatible Y shapes')
    for ychunk in range(nchunky):
        if len(set(shape[ychunk][0] for shape in allshapes)) != 1:
            raise ValueError('Incompatible Z shapes')
    if len(set(shape[2] for subshapes in allshapes for shape in subshapes)) != 1:
        raise ValueError('Incompatible X shapes')

    # compute full shape
    fullshape = [0, 0, 0]
    fullshape[0] = sum(shape[0][0] for shape in allshapes)
    fullshape[1] = sum(shape[1] for shape in allshapes[0])
    fullshape[2] = allshapes[0][0][2]

    # Prepare Zarr group
    omz = zarr.storage.DirectoryStore(out)
    omz = zarr.group(store=omz, overwrite=True)

    # Prepare chunking options
    opt = {
        'chunks': [nchannels] + [chunk] * 3,
        'dimension_separator': r'/',
        'order': 'F',
        'dtype': np.dtype(dtype).str,
        'fill_value': None,
        'compressor': make_compressor(compressor, **compressor_opt),
    }

    # write first level
    omz.create_dataset('0', shape=[nchannels, *fullshape], **opt)
    array = omz['0']
    print('Write level 0 with shape', [nchannels, *fullshape])
    for i, dirname in enumerate(all_chunks_info['dirname']):
        chunkz = all_chunks_info['z'][i] - 1
        chunky = all_chunks_info['y'][i] - 1
        planes = all_chunks_info['planes'][i]
        for j, fname in enumerate(planes['fname']):
            subz = planes['z'][j] - 1
            subc = planes['c'][j] - 1
            yx_shape = planes['yx_shape'][j]

            zstart = sum(shape[0][0] for shape in allshapes[:chunkz])
            ystart = sum(shape[1] for subshapes in allshapes for shape in subshapes[:chunky])
            print(f'Write plane ({subc}, {zstart + subz}, {ystart}:{ystart + yx_shape[0]})', end='\r')
            slicer = (
                subc,
                zstart + subz,
                slice(ystart, ystart + yx_shape[0]),
                slice(None),
            )

            f = TiffFile(fname)
            array[slicer] = f.asarray()
    print('')

    # build pyramid using median windows
    level = 0
    while any(x > 1 for x in omz[str(level)].shape[-3:]):
        prev_array = omz[str(level)]
        prev_shape = prev_array.shape[-3:]
        level += 1

        new_shape = list(map(lambda x: max(1, x//2), prev_shape))
        if all(x < chunk for x in new_shape):
            break
        print('Compute level', level, 'with shape', new_shape)
        omz.create_dataset(str(level), shape=[nchannels, *new_shape], **opt)
        new_array = omz[str(level)]

        nz, ny, nx = prev_array.shape[-3:]
        ncz = ceildiv(nz, max_load)
        ncy = ceildiv(ny, max_load)
        ncx = ceildiv(nx, max_load)

        for cz in range(ncz):
            for cy in range(ncy):
                for cx in range(ncx):

                    print(f'chunk ({cz}, {cy}, {cx}) / ({ncz}, {ncy}, {ncx})',
                          end='\r')

                    dat = prev_array[
                        ...,
                        cz*max_load:(cz+1)*max_load,
                        cy*max_load:(cy+1)*max_load,
                        cx*max_load:(cx+1)*max_load,
                    ]
                    crop = [0 if x == 1 else x % 2 for x in dat.shape[-3:]]
                    slicer = [slice(-1) if x else slice(None) for x in crop]
                    dat = dat[(Ellipsis, *slicer)]
                    pz, py, px = dat.shape[-3:]

                    dat = dat.reshape([
                        nchannels,
                        max(pz//2, 1), min(pz, 2),
                        max(py//2, 1), min(py, 2),
                        max(px//2, 1), min(px, 2),
                    ])
                    dat = dat.transpose([0, 1, 3, 5, 2, 4, 6])
                    dat = dat.reshape([
                        nchannels,
                        max(pz//2, 1),
                        max(py//2, 1),
                        max(px//2, 1),
                        -1,
                    ])
                    dat = np.median(dat, -1)

                    new_array[
                        ...,
                        cz*max_load//2:(cz+1)*max_load//2,
                        cy*max_load//2:(cy+1)*max_load//2,
                        cx*max_load//2:(cx+1)*max_load//2,
                    ] = dat

    print('')
    nblevel = level

    # Write OME-Zarr multiscale metadata
    print('Write metadata')
    multiscales = [{
        'version': '0.4',
        'axes': [
            {"name": "z", "type": "space", "unit": "micrometer"},
            {"name": "y", "type": "space", "unit": "micrometer"},
            {"name": "x", "type": "space", "unit": "micrometer"}
        ],
        'datasets': [],
        'type': 'median window 2x2x2',
        'name': '',
    }]
    multiscales[0]['axes'].insert(0, {"name": "c", "type": "channel"})

    voxel_size = list(map(float, reversed(voxel_size)))
    factor = [1] * 3
    for n in range(nblevel):
        shape = omz[str(n)].shape[-3:]
        multiscales[0]['datasets'].append({})
        level = multiscales[0]['datasets'][-1]
        level["path"] = str(n)

        # We made sure that the downsampling level is exactly 2
        # However, once a dimension has size 1, we stop downsampling.
        if n > 0:
            shape_prev = omz[str(n-1)].shape[-3:]
            if shape_prev[0] != shape[0]:
                factor[0] *= 2
            if shape_prev[1] != shape[1]:
                factor[1] *= 2
            if shape_prev[2] != shape[2]:
                factor[2] *= 2

        level["coordinateTransformations"] = [
            {
                "type": "scale",
                "scale": [1.0] + [
                    factor[0] * voxel_size[0],
                    factor[1] * voxel_size[1],
                    factor[2] * voxel_size[2],
                ]
            },
            {
                "type": "translation",
                "translation": [0.0] + [
                    (factor[0] - 1) * voxel_size[0] * 0.5,
                    (factor[1] - 1) * voxel_size[1] * 0.5,
                    (factor[2] - 1) * voxel_size[2] * 0.5,
                ]
            }
        ]
    multiscales[0]["coordinateTransformations"] = [
        {
            "scale": [1.0] * 4,
            "type": "scale"
        }
    ]
    omz.attrs["multiscales"] = multiscales

    if not nii:
        print('done.')
        return

    # Write NIfTI-Zarr header
    # NOTE: we use nifti2 because dimensions typically do not fit in a short
    # TODO: we do not write the json zattrs, but it should be added in
    #       once the nifti-zarr package is released
    shape = list(reversed(omz['0'].shape))
    shape = shape[:3] + [1] + shape[3:]    # insert time dimension
    affine = orientation_to_affine(orientation, *voxel_size)
    if center:
        affine = center_affine(affine, shape[:3])
    header = nib.Nifti2Header()
    header.set_data_shape(shape)
    header.set_data_dtype(omz['0'].dtype)
    header.set_qform(affine)
    header.set_sform(affine)
    header.set_xyzt_units(nib.nifti1.unit_codes.code['micron'])
    header.structarr['magic'] = b'nz2\0'
    header = np.frombuffer(header.structarr.tobytes(), dtype='u1')
    opt = {
        'chunks': [len(header)],
        'dimension_separator': r'/',
        'order': 'F',
        'dtype': '|u1',
        'fill_value': None,
        'compressor': None,
    }
    omz.create_dataset('nifti', data=header, shape=shape, **opt)
    print('done.')


if __name__ == "__main__":
    app()
