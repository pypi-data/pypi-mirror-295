Metadata-Version: 2.1
Name: onpod
Version: 0.0.1
Summary: Runpod abstraction layer to behave as if using a local GPU
License: MIT
Author: gabewillen
Author-email: gabewillen@gmail.com
Requires-Python: >=3.11,<4.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: pydantic (>=2.9.1,<3.0.0)
Requires-Dist: runpod (>=1.7.0,<2.0.0)
Description-Content-Type: text/markdown

# OnPod: Seamless Local and Remote AI/ML Development

OnPod is an innovative library that revolutionizes AI and machine learning development by seamlessly blending local and remote code execution. It optimizes resource usage and reduces costs while providing a development experience that feels entirely local, supporting a wide range of popular AI/ML libraries.

## Key Features

1. **Versatile Library Support**: 
   - Compatible with PyTorch, TensorFlow, Keras, and Hugging Face Transformers.
   - Extensible to support additional AI/ML libraries in the future.

2. **Transparent API**: OnPod mimics the interfaces of supported libraries, allowing for seamless integration into existing workflows.

3. **Intelligent Resource Management**: 
   - Initializes with CPU-based instances for efficient resource usage.
   - Dynamically claims GPU resources when models are moved to accelerated devices.

4. **Automatic Task Distribution**:
   - CPU-bound tasks (e.g., data preprocessing, tokenization) are performed locally.
   - GPU-intensive operations are automatically offloaded to remote RunPod instances.

5. **On-Demand Resource Allocation**: Users are charged only for the actual GPU time used, optimizing cost-efficiency.

6. **Seamless Development Experience**: Write and test code locally while leveraging the power of cloud-based resources.

## How It Works

OnPod provides proxy modules for supported AI/ML libraries that intercept operations and manage their execution:

- **Library Proxies**: Redirect operations to remote instances, handling data transfer and execution management for PyTorch, TensorFlow, Keras, and Transformers.
- **Automatic Import Handling**: Dynamically imports modules remotely when they are not configured locally.

This approach allows developers to write standard AI/ML code using their preferred libraries locally while benefiting from the computational power of cloud-based resources without manual configuration.

## Benefits

- **Cost Optimization**: Pay only for the GPU resources you actually use.
- **Resource Efficiency**: Utilize powerful GPU capabilities without the need for local high-performance hardware.
- **Flexible Development**: Develop and test AI/ML models as if working entirely locally, regardless of the chosen library.
- **Scalability**: Easily scale your computations to more powerful cloud resources when needed.
- **Library Agnostic**: Freedom to use and switch between different AI/ML libraries without changing your development workflow.

OnPod bridges the gap between local development and cloud-based high-performance computing, making advanced AI and ML development more accessible, cost-effective, and flexible across various libraries and frameworks.


