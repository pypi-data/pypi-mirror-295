Metadata-Version: 2.1
Name: transformer-checker
Version: 0.1.0
Summary: tool to visualize and check neural activations and attention patterns in transformer models that classify context-free languages
Author: Matias Molinolo
Author-email: mm231323@fi365.ort.edu.uy
Requires-Python: >=3.10,<4.0
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Requires-Dist: ipykernel (>=6.29.4,<7.0.0)
Requires-Dist: ipywidgets (>=8.1.2,<9.0.0)
Requires-Dist: jupyter (>=1.0.0,<2.0.0)
Requires-Dist: matplotlib (>=3.8.4,<4.0.0)
Requires-Dist: torch (>=2.3.0,<3.0.0)
Requires-Dist: tqdm (>=4.66.4,<5.0.0)
Requires-Dist: typer (>=0.12.3,<0.13.0)
Requires-Dist: wandb (>=0.17.1,<0.18.0)
Description-Content-Type: text/markdown

# Transformer Checker

This project is a tool to visualize and check neural activations and attention patterns in transformer models that classify context-free languages.

## Generating Data
We provide a tool to generate data for training the transformer model. The tool is located in the `dyck-k-generation` directory. To use the tool, run the following command:

```bash
python -m dyck_k_generator.generator --n n --k k --max_length max_length --balanced balanced --file
```

where `n` is the number of samples, `k` is the Dyck-k language, `max_length` is the maximum length of the strings, `balanced` is a float between 0 and 1 that represents the percentage of balanced strings, and `file` is flag that represents whether to save the data to a file or return it to a variable.

Default values are `n=500_000`, `k=3`, `max_length=1025` and `balanced=0.5`.

