Metadata-Version: 2.1
Name: ironaai
Version: 0.1.12
Summary: Irona AI Python SDK
Home-page: https://irona.ai/
Author: Irona AI
Author-email: support@irona.ai
Requires-Python: >=3.10,<3.13
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Provides-Extra: create
Requires-Dist: aiohttp (>=3.9.3,<4.0.0)
Requires-Dist: langchain (>=0.2.7) ; extra == "create"
Requires-Dist: langchain-anthropic (>=0.1.20,<0.2.0) ; extra == "create"
Requires-Dist: langchain-cohere (>=0.1.9,<0.2.0) ; extra == "create"
Requires-Dist: langchain-community (>=0.2.16,<0.3.0) ; extra == "create"
Requires-Dist: langchain-google-genai (>=1.0.7) ; extra == "create"
Requires-Dist: langchain-mistralai (>=0.1.10,<0.2.0) ; extra == "create"
Requires-Dist: langchain-openai (>=0.1.16,<0.2.0) ; extra == "create"
Requires-Dist: langchain-together (>=0.1.4,<0.2.0) ; extra == "create"
Requires-Dist: litellm (>=1.30.0,<2.0.0)
Requires-Dist: pandas (>=2.2.2,<3.0.0)
Requires-Dist: pydantic-partial (>=0.5.5,<0.6.0)
Requires-Dist: python-dotenv (>=1.0.1,<2.0.0)
Requires-Dist: requests (>=2.31)
Project-URL: Documentation, https://ironaai.mintlify.app/
Project-URL: Repository, https://github.com/Irona-AI/ironaai-python
Description-Content-Type: text/markdown

# Getting started with Irona AI

Irona AI is an AI model router that automatically determines which LLM is best-suited to respond to any query, improving LLM output quality by combining multiple LLMs into a **meta-model** that learns when to call each LLM.

# Key features

- **[Maximize output quality](https://ironaai.readme.io/docs/quickstart)**: Irona AI [outperforms every foundation model](https://ironaai.readme.io/docs/benchmark-performance) on major evaluation benchmarks by always calling the best model for every prompt.
- **[Reduce cost and latency](https://ironaai.readme.io/docs/cost-and-latency-tradeoffs)**: Irona AI lets you define intelligent cost and latency tradeoffs to efficiently leverage smaller and cheaper models without degrading quality.
- **[Train your own router](https://ironaai.readme.io/docs/router-training-quickstart)**: Irona AI lets you train your own custom routers optimized to your data and use case.
- **[Python](https://python.ironaai.ai/), [TypeScript](https://www.npmjs.com/package/ironaai), and [REST API](https://ironaai.readme.io/reference/api-introduction) support**: Irona AI works across a variety of stacks.

# Installation

**Python**: Requires **Python 3.10+**. Itâ€™s recommended that you create and activate a [virtualenv](https://virtualenv.pypa.io/en/latest/) prior to installing the package. For this example, we'll be installing the optional additional `create` dependencies, which you can learn more about [here](https://ironaai.readme.io/docs/model_select-vs-create).

```shell
pip install ironaai
```

# Setting up

Create a `.env` file with your [Irona AI API key](https://app.ironaai.ai/keys) and the [API keys of the models](https://ironaai.readme.io/docs/api-keys) you want to route between:

```shell
IRONAAI_API_KEY = "YOUR_IRONAAI_API_KEY"
OPENAI_API_KEY = "YOUR_OPENAI_API_KEY"
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_API_KEY"
```

# Sending your first Irona AI API request

Create a new file in the same directory as your `.env` file and copy and run the code below (you can toggle between  Python and TypeScript in the top left of the code block):

```python
from ironaai import IronaAI

# Define the Irona AI routing client
client = IronaAI()

# The best LLM is determined by Irona AI based on the messages and specified models
result, session_id, provider = client.chat.completions.create(
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Concisely explain merge sort."}  # Adjust as desired
    ],
    model=['openai/gpt-3.5-turbo', 'openai/gpt-4o', 'anthropic/claude-3-5-sonnet-20240620']
)

print("ND session ID: ", session_id)   # A unique ID of Irona AI's recommendation
print("LLM called: ", provider.model)  # The LLM routed to
print("LLM output: ", result.content)  # The LLM response
```

